# -*- coding: utf-8 -*-
"""Project 1-The Simpsons Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wDh7F51ag7zlYALdzXSJcEWb1FCa3E2v
"""

!pip install opendatasets

import opendatasets as od
import numpy as np
import os
import shutil
import random
import cv2
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D, Conv2D, Dropout, BatchNormalization
from tensorflow.keras.optimizers import SGD, Adam, RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

od.download("https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset", force=True)
#shigadsouza
#040fd7cbd8eb7c91b8338c30ac12e508

"""# Preparaing Data"""

dir_path = '/content/the-simpsons-characters-dataset/simpsons_dataset'


for x in os.listdir(dir_path):
  count = 0
  folder_name = x
  folder_path = os.path.join(dir_path, x)
  for i in os.listdir(folder_path):
    count += 1
  print(f'{folder_name}: {count} images')

df = pd.read_csv('/content/the-simpsons-characters-dataset/number_pic_char.csv', index_col=0)
df

"""As can be seen in the above table most of the characters have images less than 600 while others have more than 800. Due to this the model wont be able to learn between different characters properly.
To prevent from developing a poor model, only characters with more than 800 images are taken
"""

data = df[df['total'] > 800]
data

name_list = [name.lower() for name in data['name']]
names = []
for name in name_list:
  if ' ' in name:
    names.append(name.replace(' ', '_'))
  else:
    names.append(name)
names.sort()
print(names)

path = '/content/simpsons_dataset'
# os.makedirs('simpsons_dataset')

for x in os.listdir(dir_path):
  if x in names:
    folder_path = os.path.join(path, x)
    os.makedirs(folder_path)
    for y in os.listdir(os.path.join(dir_path, x)):
      shutil.move(os.path.join(dir_path, x, y), folder_path)

for x in os.listdir(path):
  count = 0
  folder_name = x
  folder_path = os.path.join(path, x)
  for i in os.listdir(folder_path):
    count += 1
  print(f'{folder_name}: {count} images')

folder = '/content/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset'
new_path = '/content/simpsons_test_set'
os.makedirs(new_path)

for y in names:
    new_folder = os.path.join(new_path, y)
    os.makedirs(new_folder)

for x in os.listdir(folder):
  for y in names:
    if y in x:
      image = os.path.join(folder, x)
      new_folder = os.path.join(new_path, y)
      shutil.move(image, new_folder)

for x in os.listdir(new_path):
  count = 0
  folder_name = x
  folder_path = os.path.join(new_path, x)
  for i in os.listdir(folder_path):
    count += 1
  print(f'{folder_name}: {count} images')

dir_path = '/content/simpsons_dataset'

num = 0
characters_list = []

for i in os.listdir(dir_path):
  characters_list.append(i)

characters_list.sort()

characters_list

def load_random_img(dir, folders):
  plt.figure(figsize=(20,20))
  i=0
  
  for character in folders:
    i+=1
    plt.subplot(13, 4, i)
    character_foler = os.path.join(f'{dir}/{character}')
    image = random.choice(os.listdir(f'{dir}/{character}'))
    image_path = os.path.join(character_foler, image)
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.title(character)
    plt.imshow(img)

load_random_img(dir_path, characters_list)

"""# Preprocessing Data"""

train_dir = '/content/simpsons_dataset'
test_dir = '/content/simpsons_test_set'

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2, 
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)
valid_datagen = ImageDataGenerator(
    rescale = 1./255,
    validation_split=0.2
)
test_datagen = ImageDataGenerator(rescale=1./255)

inp_shape = (128, 128)
batch_size = 32
input_shape = (128, 128, 3)
epochs=50
num_class = len(names)
print(num_class)

train_gen = train_datagen.flow_from_directory(
    train_dir,
    target_size=inp_shape,
    batch_size = batch_size,
    class_mode='categorical',
    subset='training'
)

valid_gen = valid_datagen.flow_from_directory(
    train_dir,
    target_size=inp_shape,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

test_gen = test_datagen.flow_from_directory(
    test_dir,
    target_size=inp_shape,
    batch_size=batch_size,
    class_mode='categorical', 
    shuffle=False
)

"""# Creating CNN"""

keras.backend.clear_session()

# from tensorflow.keras.applications import MobileNetV2, VGG16

# vgg = VGG16(
#     weights='imagenet',
#     include_top=False,
#     input_shape=input_shape,
#     pooling='avg'
# )
# vgg.summary()
# vgg.trainable = False

model = Sequential()
model.add(Conv2D(input_shape=input_shape,filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))
model.add(BatchNormalization())


model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))
model.add(BatchNormalization())


model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))
model.add(BatchNormalization())


model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))
model.add(BatchNormalization())


model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))
model.add(BatchNormalization())


model.add(Flatten())
model.add(Dense(units=4096,activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(units=4096,activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(num_class, activation="softmax"))

print(model.summary())

model.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=0.0001),
              metrics='accuracy')

best = ModelCheckpoint('Best_Model.h5', save_best_only=True)
stop = EarlyStopping(monitor= 'val_accuracy',patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',patience = 3,min_delta = 0.00001)

history = model.fit(
    train_gen,
    steps_per_epoch=int(13276 /batch_size),
    epochs=epochs,
    validation_data=valid_gen,
    validation_steps=int(3311 /batch_size),
   callbacks=[best, stop, reduce_lr]  
 )

pd.DataFrame(history.history).plot(figsize=(12, 5))
plt.show()

"""# Model Evaluation"""

model.evaluate(test_gen)

from sklearn.metrics import classification_report, confusion_matrix

preds=model.predict(test_gen)  
labels=test_gen.labels
classes=list(test_gen.class_indices.keys()) 
pred_list=[ ] 
true_list=[]
for i, p in enumerate (preds):
    index=np.argmax(p)
    pred_list.append(classes[index])
    true_list.append(classes[labels[i]])
y_pred=np.array(pred_list)
y_true=np.array(true_list)
clr = classification_report(y_true, y_pred, target_names=classes)
print("Classification Report:\n----------------------\n", clr)
cm = confusion_matrix(y_true, y_pred )

import seaborn as sns

fig, ax = plt.subplots(figsize=(7, 7))
sns.heatmap(cm, linewidths =2, annot=True)
plt.show()

img_path = '/content/simpsons_test_set/bart_simpson/bart_simpson_7.jpg'
image = cv2.imread(img_path)
img = cv2.resize(image, (128, 128)).astype('float32') / 255.0
img = np.expand_dims(img, axis=0)

classes=np.argmax(model.predict(img),axis=1)
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.axis('off')
print(f'Predicted character is = {characters_list[classes[0]]}')

